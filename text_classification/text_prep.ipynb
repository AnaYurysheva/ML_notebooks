{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "421db3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5015869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1af7df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer, stopwords\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b3e54fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 10s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e5825a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna('')\n",
    "data['text'] = data['title'] + ' ' + data['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42b47535",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = list(punctuation + '—»«0123456789')\n",
    "punct = tuple(punct)\n",
    "stop_list = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96a93c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text']\n",
    "y = data['Category']\n",
    "X_full, X_train, y_full, y_train = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4226c",
   "metadata": {},
   "source": [
    "To make process easy for pc, will work only with 20% of data (about 800 000 adds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "591a312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text_fix = []\n",
    "    stemmer = SnowballStemmer('russian')\n",
    "    for text_proba in tqdm(X_train):\n",
    "        text_proba = text_proba.lower()\n",
    "        text_proba = word_tokenize(text_proba)\n",
    "        text_proba = [x for x in text_proba if x not in stop_list and not x.startswith(punct)]\n",
    "        text_proba = [stemmer.stem(w) for w in text_proba]\n",
    "        text_fix.append(text_proba)\n",
    "    return text_fix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2926f5c7",
   "metadata": {},
   "source": [
    "Adds preprocess (lower case, tokenize and deliting most frequent unuseful words from stopwords, stemming with Snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dfeaaab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['эб',\n",
       " 'renault',\n",
       " 'nissan',\n",
       " 'dci',\n",
       " 'delphi',\n",
       " 'год',\n",
       " 'комплект',\n",
       " 'эб',\n",
       " 'мозг',\n",
       " 'компьютер',\n",
       " 'мотор',\n",
       " 'двигател',\n",
       " 'коммутатор',\n",
       " 'электрон',\n",
       " 'блок',\n",
       " 'управлен',\n",
       " 'двигател',\n",
       " 'компьютер',\n",
       " 'мотор',\n",
       " 'рен',\n",
       " 'kangoo',\n",
       " 'канг',\n",
       " 'канг',\n",
       " 'мега',\n",
       " 'сценик',\n",
       " 'laguna',\n",
       " 'лагун',\n",
       " 'clio',\n",
       " 'кли',\n",
       " 'nissan',\n",
       " 'micra',\n",
       " 'нисса',\n",
       " 'микр',\n",
       " 'друг',\n",
       " 'рен',\n",
       " 'нисса',\n",
       " 'дизел',\n",
       " 'топливн',\n",
       " 'аппаратур',\n",
       " 'делф',\n",
       " 'б.у',\n",
       " 'оригина',\n",
       " 'отличн',\n",
       " 'состоян',\n",
       " 'проверен',\n",
       " 'гарант',\n",
       " 'запчаст',\n",
       " 'предоставля',\n",
       " 'гарант',\n",
       " 'возможн',\n",
       " 'установк',\n",
       " 'подробн',\n",
       " 'раздел',\n",
       " 'компан',\n",
       " 'дорож',\n",
       " 'сво',\n",
       " 'репутац',\n",
       " 'торгу',\n",
       " 'легальн',\n",
       " 'контрактн',\n",
       " 'запчаст',\n",
       " 'дета',\n",
       " 'налич',\n",
       " 'таможен',\n",
       " 'документ',\n",
       " 'запчаст',\n",
       " 'автомобил',\n",
       " 'пробег',\n",
       " 'росс',\n",
       " 'отправля',\n",
       " 'друг',\n",
       " 'регион',\n",
       " 'таможен',\n",
       " 'союз',\n",
       " 'транспортн',\n",
       " 'компан',\n",
       " 'наложн',\n",
       " 'платеж']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_try = X_full[0]\n",
    "x_try = x_try.lower()\n",
    "x_try = word_tokenize(x_try)\n",
    "\n",
    "text_proba = [x for x in x_try if x not in stop_list and not x.startswith(punct)]\n",
    "stemmer = SnowballStemmer('russian')\n",
    "text_proba = [stemmer.stem(w) for w in text_proba]\n",
    "text_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76de94e",
   "metadata": {},
   "source": [
    "try cell for one ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c7c5ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 846809/846809 [54:48<00:00, 257.54it/s]  \n"
     ]
    }
   ],
   "source": [
    "X_train = preprocess(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "38ee31a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['горк',\n",
       " 'башн',\n",
       " 'прод',\n",
       " 'горк',\n",
       " 'башн',\n",
       " 'отличн',\n",
       " 'состоян',\n",
       " 'горк',\n",
       " 'изготовл',\n",
       " 'высококачествен',\n",
       " 'термостойк',\n",
       " 'пластик',\n",
       " 'легк',\n",
       " 'собира',\n",
       " 'элемент',\n",
       " 'конструкц',\n",
       " 'соединя',\n",
       " 'помощ',\n",
       " 'гаек',\n",
       " 'внутр',\n",
       " 'башн',\n",
       " 'ребенок',\n",
       " 'устро',\n",
       " 'домик',\n",
       " 'занима',\n",
       " 'площад',\n",
       " 'м',\n",
       " 'высот',\n",
       " 'длин',\n",
       " 'скат',\n",
       " 'м',\n",
       " 'допустим',\n",
       " 'нагрузк',\n",
       " 'кг',\n",
       " 'рубл']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[846808]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79f80f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/example.txt', 'w', encoding='utf-8') as f:\n",
    "    for describe in X_train:\n",
    "        f.write(' '.join(describe) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/example_cat.txt', 'w', encoding='utf-8') as f:\n",
    "    for category in y_train:\n",
    "        f.write(str(category) + '\\n')\n",
    "        \n",
    "cat_df = pd.DataFrame(data[['Category','Category_name']])\n",
    "cat_df = cat_df.drop_duplicates()\n",
    "cat_df.to_csv(r'data/ex_dict_cat.txt', header=None, index=None, sep=' ', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2225d16f",
   "metadata": {},
   "source": [
    "##### Prepared text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cfa96fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "846809"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/example.txt', 'r', encoding='utf-8') as f:\n",
    "    reviews = f.read()\n",
    "\n",
    "all_reviews = reviews.split(\"\\n\")\n",
    "all_reviews.pop(846809)\n",
    "len(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff62b479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "846809"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/example_cat.txt', 'r', encoding='utf-8') as f:\n",
    "    labels = f.read()\n",
    "\n",
    "all_labels = labels.split(\"\\n\")\n",
    "all_labels.pop(846809)\n",
    "len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed553f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = [int(x) for x in all_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328edb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['колон точен элемент интерьер декор люб помещен очен легк разнообраз облагород украс колон интерьер использова сдвоен элемент зависим площад одн комнат могут располага сраз две конструкц обязательн должн выполн одн стил будут создава един гармоничн композиц перетягива вниман одн другую./ колон использова опорн элемент беседок',\n",
       " 'skoda octavia усп приобрест нов skoda специальн услов налич официальн дилер крона-авт гарант лучш цен улучш люб предложен покупк šкод крона-авт эт ✓ кред šкод finance./ ✔ пакет опц подарок ✔ помощ дорог подарок/ ✔ автомоб налич птс/ ✔ реальн выгод денежн выражен сравнен комплектац конкурент ✓ сдач зачет сво автомобил предоставля дополнительн выгод ✓ сможет комфорт выбра подходя автомобил соверш выгодн покупк течен дня звон ответ ваш вопрос легкосплавн диски/ электрообогр лобов стекла/ противотума фары/ накладк пороги/ охлажда перчаточн ящик/ стальн диски/ передн центральн подлокотник/ bluetooth',\n",
       " 'iphone комплект телефон включа т.к нет батарейк айклауд отвяж']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reviews[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9d4b2",
   "metadata": {},
   "source": [
    "### Lineral model attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a768596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a5530e",
   "metadata": {},
   "source": [
    "##### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "281d96a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8640072743590652\n"
     ]
    }
   ],
   "source": [
    "cnt_vec = CountVectorizer()\n",
    "X = cnt_vec.fit_transform(all_reviews)\n",
    "\n",
    "X_s_train, X_s_test, y_s_train, y_s_test = train_test_split(X, all_labels, test_size=0.2, random_state=1)\n",
    "\n",
    "sgd_m = SGDClassifier(max_iter=1000)\n",
    "sgd_m.fit(X_s_train, y_s_train)\n",
    "prediction = sgd_m.predict(X_s_test)\n",
    "\n",
    "print(accuracy_score(prediction, y_s_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f052e6b",
   "metadata": {},
   "source": [
    "##### Tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "93c37324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8378325716512559\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(all_reviews)\n",
    "\n",
    "X_s_train, X_s_test, y_s_train, y_s_test = train_test_split(X, all_labels, test_size=0.2, random_state=1)\n",
    "\n",
    "sgd_m = SGDClassifier()\n",
    "sgd_m.fit(X_s_train, y_s_train)\n",
    "prediction = sgd_m.predict(X_s_test)\n",
    "\n",
    "print(accuracy_score(prediction, y_s_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "648217ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8377085768944628\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer(max_df=0.9)\n",
    "X = tfidf_vec.fit_transform(all_reviews)\n",
    "\n",
    "X_s_train, X_s_test, y_s_train, y_s_test = train_test_split(X, all_labels, test_size=0.2, random_state=1)\n",
    "\n",
    "sgd_m = SGDClassifier()\n",
    "sgd_m.fit(X_s_train, y_s_train)\n",
    "prediction = sgd_m.predict(X_s_test)\n",
    "\n",
    "print(accuracy_score(prediction, y_s_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a350c",
   "metadata": {},
   "source": [
    "##### Tuned SGD with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35496fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vec = CountVectorizer()\n",
    "X = cnt_vec.fit_transform(all_reviews)\n",
    "X_s_train, X_s_test, y_s_train, y_s_test = train_test_split(X, all_labels, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b54f768e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:38<01:16, 38.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8640781285058041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 67%|██████▋   | 2/3 [01:29<00:46, 46.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8399995276390217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [02:21<00:00, 47.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8397869651988049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss = ['hinge', 'log_loss']\n",
    "penalty = ['l1', 'l2', 'elasticnet']\n",
    "alpha = [0.0001,0.001, 0.01, 0.1, 1, 10]\n",
    "fit_intercept = [False, True]\n",
    "\n",
    "for i in tqdm(loss):\n",
    "    sgd_m = SGDClassifier(max_iter=1000, n_jobs=-1, loss=i)\n",
    "    sgd_m.fit(X_s_train, y_s_train)\n",
    "    prediction = sgd_m.predict(X_s_test)\n",
    "    print(accuracy_score(prediction, y_s_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd2b5c3",
   "metadata": {},
   "source": [
    "Best loss = hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34fd4bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [02:46<05:33, 166.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7727176107981719 penalty= l1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 67%|██████▋   | 2/3 [03:27<01:32, 92.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8641430781403149 penalty= l2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:50<00:00, 96.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8486968741512264 penalty= elasticnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(penalty):\n",
    "    sgd_m = SGDClassifier(max_iter=1000, n_jobs=-1, penalty=i, loss='hinge')\n",
    "    sgd_m.fit(X_s_train, y_s_train)\n",
    "    prediction = sgd_m.predict(X_s_test)\n",
    "    print(accuracy_score(prediction, y_s_test), 'penalty=', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5509977",
   "metadata": {},
   "source": [
    "Best penalty = l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc421eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:38<03:14, 38.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8642965954582492 alpha= 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|███▎      | 2/6 [01:13<02:24, 36.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8412453797191815 alpha= 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 50%|█████     | 3/6 [01:46<01:45, 35.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7764138354530532 alpha= 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 67%|██████▋   | 4/6 [02:58<01:39, 49.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5872627862212303 alpha= 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 83%|████████▎ | 5/6 [04:11<00:57, 57.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4468475809213401 alpha= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [05:02<00:00, 50.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3940435280641466 alpha= 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(alpha):\n",
    "    sgd_m = SGDClassifier(max_iter=1000, n_jobs=-1, loss='hinge', penalty='l2' , alpha=i)\n",
    "    sgd_m.fit(X_s_train, y_s_train)\n",
    "    prediction = sgd_m.predict(X_s_test)\n",
    "    print(accuracy_score(prediction, y_s_test), 'alpha=', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d34ad",
   "metadata": {},
   "source": [
    "Best alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6247bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:39<00:39, 39.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8710867845207307 fit_intersept= False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:19<00:00, 39.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8641903142381407 fit_intersept= True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(fit_intercept):\n",
    "    sgd_m = SGDClassifier(max_iter=1000, n_jobs=-1, loss='hinge', penalty='l2' , alpha=0.0001 ,fit_intercept=i)\n",
    "    sgd_m.fit(X_s_train, y_s_train)\n",
    "    prediction = sgd_m.predict(X_s_test)\n",
    "    print(accuracy_score(y_s_test, prediction), 'fit_intersept=', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc55aff",
   "metadata": {},
   "source": [
    "##### best tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc7e9c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8707266092748078\n"
     ]
    }
   ],
   "source": [
    "sgd_m = SGDClassifier(max_iter=1000, n_jobs=-1, loss='hinge', penalty='l2', alpha=0.0001, fit_intercept=False)\n",
    "sgd_m.fit(X_s_train, y_s_train)\n",
    "\n",
    "prediction = sgd_m.predict(X_s_test)\n",
    "print(accuracy_score(y_s_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f62c5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d49cae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86813158 0.86872498 0.867945  ]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(sgd_m, X_s_train, y_s_train, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d919ea7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8569809  0.85657349 0.8544833 ]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(sgd_m, X_s_test, y_s_test, cv=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b8331",
   "metadata": {},
   "source": [
    "### NLP try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67f18433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9cb3ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(846809, 846809, 36)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_reviews), len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c958466",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(all_reviews)\n",
    "all_words = text.split()\n",
    "    \n",
    "corpus = Counter(all_words)\n",
    "corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:5000]\n",
    "\n",
    "vocab_to_int = {w:i+1 for i,w in enumerate(corpus_)}\n",
    "encoded_reviews = []\n",
    "for sent in all_reviews:\n",
    "    encoded_reviews.append([vocab_to_int[word] for word in sent.lower().split() \n",
    "                                  if word in vocab_to_int.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8abcd2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844571 844571\n"
     ]
    }
   ],
   "source": [
    "encoded_labels = np.array( [label for idx, label in enumerate(all_labels) if len(encoded_reviews[idx]) > 0] )\n",
    "encoded_reviews = [review for review in encoded_reviews if len(review) > 0]\n",
    "\n",
    "print(len(encoded_labels), len(encoded_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c07071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = pd.get_dummies(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58309697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_text(encoded_reviews, seq_length):\n",
    "    reviews = []\n",
    "    for review in encoded_reviews:\n",
    "        if len(review) >= seq_length:\n",
    "            reviews.append(review[:seq_length])\n",
    "        else:\n",
    "            reviews.append([0]*(seq_length-len(review)) + review)\n",
    "        \n",
    "    return np.array(reviews)\n",
    "\n",
    "\n",
    "padded_reviews = pad_text(encoded_reviews, seq_length = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b061770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(760113, 250) (760113, 50)\n",
      "(84458, 250) (84458, 50)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(padded_reviews,encoded_labels, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b81f4293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 250, 100)          501000    \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  (None, 250, 100)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 586,450\n",
      "Trainable params: 586,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 5010\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=250))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(50, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "914c528c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5345/5345 [==============================] - 12011s 2s/step - loss: 0.8924 - accuracy: 0.7515 - val_loss: 0.5552 - val_accuracy: 0.8389\n",
      "Epoch 2/5\n",
      "5345/5345 [==============================] - 11636s 2s/step - loss: 0.5225 - accuracy: 0.8446 - val_loss: 0.4845 - val_accuracy: 0.8545\n",
      "Epoch 3/5\n",
      "5345/5345 [==============================] - 11692s 2s/step - loss: 0.4712 - accuracy: 0.8564 - val_loss: 0.4633 - val_accuracy: 0.8591\n",
      "Epoch 4/5\n",
      "5345/5345 [==============================] - 11727s 2s/step - loss: 0.4462 - accuracy: 0.8621 - val_loss: 0.4544 - val_accuracy: 0.8610\n",
      "Epoch 5/5\n",
      "5345/5345 [==============================] - 11810s 2s/step - loss: 0.4292 - accuracy: 0.8662 - val_loss: 0.4461 - val_accuracy: 0.8627\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb6224",
   "metadata": {},
   "source": [
    "Ok. Model was teaching about 10 hours but no improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f44ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
